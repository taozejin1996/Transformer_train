# -*- coding: utf-8 -*-
"""Transformer_Train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T0gxjxoBDXvWntT-Vd950jLyU95Jy2c8
"""

!pip install -U spacy

!python -m spacy download en_core_web_lg

!python -m spacy download en_core_web_sm

!pip install simpletransformers

import json
import pandas as pd
import spacy
from tqdm import tqdm
import torch
import torch.nn as nn
import os
from google.colab import drive
from simpletransformers.model import TransformerModel



PATH = '/content/drive/MyDrive/2012_USPTO.json'
patent_file = open(PATH).read()
patent_data = json.loads(patent_file)
nlp = spacy.load('en_core_web_lg')

def transform_label_to_code(label_list : list):
      result = []
      for label in label_list:
            idx = unique_label_dict[label]
            result.append(idx)
            return result

def transform_token_to_idx(text : str):
      doc = nlp(text)
      result = []
      for token in doc:
            result.append(vocab_dict[str(token)])
      return result

def put_eos_token(token_list : list):
      token_list.append(16510)
      return token_list


### Training Data
vocab_list = []
patent_document_list = []
token_id_list = []
label_list = []

for patent_document in tqdm(patent_data):
    patent_document_list=[]
    abstract = patent_document["Abstract"]
    title = patent_document["Title"]
    subclass_labels = patent_document["Subclass_labels"]
    patent_document_list.append(
      [title, abstract, subclass_labels])
    df = pd.DataFrame(patent_document_list, columns=["title","abstract","subclass"])
    df["subclass_idx"] = df["subclass"].map(transform_label_to_code)
    if not df["subclass_idx"][0]:
      print("a")
      label_list.append(-1)
    else:
      label_list.append(df["subclass_idx"][0][0])
    doc = nlp(df["abstract"][0])
    df["full_text"] = df["title"]+""+df["abstract"]
    doc = nlp(df["full_text"][0])
    for token in doc:
      vocab_list.append(token.text)
    vocab_dict = {key : idx for idx, key in enumerate(set(vocab_list))}
    df["token_id_list"] = df["full_text"].map(transform_token_to_idx)
    vocab_dict["<eos>"] = 16510
    df["token_id_list"].map(put_eos_token)
    token_id_list.append([df["token_id_list"][0]])

from google.colab import drive
drive.mount('/content/drive')

from simpletransformers.model import TransformerModel
df=pd.read_json('/content/drive/MyDrive/2012_USPTO.json')
unique_label_set = set([code for codes in df["Subclass_labels"].tolist() for code in codes])
unique_label_dict = { key: idx for idx, key in enumerate(unique_label_set)}
label_num = len(unique_label_dict)


train_data = {'Text':token_id_list[:int(len(token_id_list)*0.7)],"Label":label_list[:int(len(label_list)*0.7)]}
train_data = pd.DataFrame(train_data)

val_data = {'Text':token_id_list[int(len(token_id_list)*0.7):],"Label":label_list[int(len(label_list)*0.7):]}
val_data = pd.DataFrame(train_data)

model = TransformerModel('roberta', 'roberta-base', num_labels=label_num,use_cuda=False, 
args={'learning_rate':1e-5, 'num_train_epochs': 2, 
'reprocess_input_data': True, 'overwrite_output_dir': True})

model.train_model(train_data)

from sklearn.metrics import f1_score, accuracy_score,precision_score,recall_score

def f1_multiclass(labels, preds):
  return f1_score(labels, preds, average='micro')

result, model_outputs, wrong_predictions = model.eval_model(val_data, f1=f1_multiclass, acc=accuracy_score)

print("result:",result)

def precision(labels, preds):
  return precision_score(labels,preds,average='macro')
  

result, model_outputs, wrong_predictions = model.eval_model(val_data, precision=precision, acc=accuracy_score)

print("result:",result)


def recall(labels, preds):
  return recall_score(labels,preds,average='macro')
  

result, model_outputs, wrong_predictions = model.eval_model(val_data, reacall=recall, acc=accuracy_score)

print("result:",result)